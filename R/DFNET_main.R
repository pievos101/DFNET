#' Use random forest algorithm to perform feature selection with respect to
#' given modules.
#'
#' @param modules a list of modules (node vectors)
#' @param features the features to train on
#' @param mmt the multi-modal target vector as returned by
#' \code{multi_modal_target(features)}
#' @return a decision forest with one tree per module in modules
DFNET_make_forest <- function(modules, features, mmt) {
    if (missing(mmt)) {
        mmt <- multi_modal_target(features)
    }
    selected_nodes_weights <- lapply(modules, function(module) {
        as.numeric(table(module))
    })
    target <- mmt$target

    lapply(1:length(modules), function(m) {
        unique_nodes <- sort(unique(modules[[m]]))
        unique_nodes_weights <- selected_nodes_weights[[m]]
        weights <- unique_nodes_weights

        if (mmt$is.multi_modal) {
            mm_data <- features[[1]][, unique_nodes]
            for (column in 2:length(features)) {
                mm_data <- cbind(mm_data, features[[column]][, unique_nodes])
                weights <- c(weights, unique_nodes_weights)
            }
        } else {
            mm_data <- features[, unique_nodes]
        }
        mm_data <- as.data.frame(cbind(mm_data, target))

        # Perform feature selection
        ranger(
            dependent.variable.name = "target",
            data = mm_data,
            split.select.weights = weights / sum(weights),
            verbose = FALSE,
            classification = TRUE,
            importance = "impurity",
            num.trees = 1,
            mtry = dim(mm_data)[2] - 1,
            replace = TRUE
        )
    })
}

#' Initialize the decision forest network.
#'
#' @param graph the graph to train the network on
#' @param features the features to use for training
#' @param ntrees how many trees should be generated in the initial step
#' and each following iteration
#' @param walk.depth how many nodes should be selected per tree.
#' If not a number, \code{ceiling(sqrt(length(V(graph))))} will be used instead.
#' @return a list of shape \code{(graph, features, walk.depth, trees,
#' modules.nodes, modules.auc)}, where \code{graph} and \code{features} are
#' as in the input, \code{walk.depth} is expanded to a vector with \code{ntrees}
#' elements, \code{modules.nodes} are randomly selected modules,
#' \code{trees} are decision trees learned for these nodes as per
#' \code{DFNET_make_forest}, and \code{modules.auc} is the area under
#' curve w.r.t. the "target" column of \code{features} for each tree in
#' \code{trees}.
DFNET_init <- function(graph, features, ntrees = 100, walk.depth = NaN) {
    mmt <- multi_modal_target(features)

    nodes <- V(graph)
    n.nodes <- length(nodes)

    if (is.na(walk.depth)) {
        walk.depth <- ceiling(sqrt(n.nodes))
    }
    walk.depth <- rep_len(walk.depth, ntrees)

    count <- 1
    selected_nodes <- list()
    repeat {
        sampled.nodes <- sample(nodes, (ntrees + 1 - count), replace = TRUE)
        for (sn in sampled.nodes) {
            depth <- walk.depth[[count]]
            selected_nodes[[count]] <- as.numeric(random_walk(graph, sn, depth))
            # Pick only walks of maximal length
            if (length(selected_nodes[[count]]) >= walk.depth[[count]]) {
                count <- count + 1
            }
        }
        if (count > ntrees) {
            break
        }
    }

    decision_trees <- DFNET_make_forest(selected_nodes, features, mmt)

    return(list(
        graph = graph,
        features = features,
        walk.depth = walk.depth,
        trees = decision_trees,
        modules.nodes = selected_nodes,
        modules.auc = auc_per_tree(decision_trees, mmt$target)
    ))
}

#' Perform iterations on the decision forest network.
#'
#' @param state a state as generated by \code{DFNET_init} or
#' \code{DFNET_iterate}.
#' @param niter the number of iterations to run.
#' @param offset an offset for the iteration count (used for logging only)
#' @param min.walk_depth the minimal random walk depth in each iteration
#' @return the updated state, see \code{DFNET_init} for its shape.
#' @examples
#' \dontrun{
#' state <- DFNET_init(...)
#' offset <- 0
#' # we want to be "significantly" better than random guessing
#' while (any(state$modules.auc < 0.6)) {
#'     state <- DFNET_iterate(state, niter = 10)
#'     offset <- offset + 10
#' }
#' }
#'
DFNET_iterate <- function(state, niter = 200, offset = 0, min.walk_depth = 2) {
    features <- state$features
    mmt <- multi_modal_target(features)

    all.trees <- state$trees
    old.walk_depth <- state$walk.depth
    old.modules <- state$modules.nodes
    old.auc <- state$modules.auc

    # From DFNET_init we have one module per tree, let's keep this invariant
    ntrees <- length(state$modules.nodes)

    iter.min <- offset + 1
    iter.max <- offset + niter
    for (iter in iter.min:iter.max) {
        # @FIXME: Use logger?
        cat(iter, " of ", iter.max, "greedy steps\n")

        ids_keep <- sample(1:length(old.auc), prob = old.auc, replace = TRUE)
        kept_modules <- old.modules[ids_keep]
        walk.depth <- old.walk_depth[ids_keep]

        start_nodes <- sapply(kept_modules, function(module) {
            sample(module, 1)
        })

        modules <- lapply(1:length(start_nodes), function(sn) {
            as.numeric(random_walk(state$graph, start_nodes[sn], walk.depth[sn]))
        })

        old.trees <- tail(all.trees, ntrees)
        trees <- DFNET_make_forest(modules, features, mmt)

        modules.auc <- auc_per_tree(trees, mmt$target)

        ids.shrink <- which(modules.auc >= old.auc)
        ids.shrink_not <- which(modules.auc < old.auc)

        if (length(ids.shrink) > 0) {
            old.auc[ids.shrink] <- modules.auc[ids.shrink]
            old.modules[ids.shrink] <- modules[ids.shrink]
            old.walk_depth[ids.shrink] <- walk.depth[ids.shrink]
            # hotfix
            trees[ids.shrink] <- trees[ids.shrink]
        }
        if (length(ids.shrink_not) > 0) {
            # @FIXME: does this need copying?
            old.auc[ids.shrink_not] <- old.auc[ids.shrink_not]
            old.modules[ids.shrink_not] <- old.modules[ids.shrink_not]
            old.walk_depth[ids.shrink_not] <- old.walk_depth[ids.shrink_not]
            # hotfix
            trees[ids.shrink_not] <- old.trees[ids.shrink_not]
        }

        all.trees = c(all.trees, trees)

        old.walk_depth[old.walk_depth < min.walk_depth] <- min.walk_depth
    }

    return(list(
        graph = state$graph,
        features = state$features,
        walk.depth = old.walk_depth,
        trees = all.trees,
        modules.nodes = old.modules,
        modules.auc = old.auc
    ))
}

#' Construct a new decision forest and run some iterations on it.
#'
#' @param DFNET_graph a list, whose first element is a graph and whose second
#' element is a matrix of features
#' @param ntrees how many trees should be generated initially and per iteration
#' @param niter how many iterations to run
#' @param init.mtry how many nodes to select per initial tree
#' @return a list of shape \code{(DFNET_graph, DFNET_trees, DFNET_MODULES,
#' DFNET_MODULES_AUC)}, where \code{DFNET_graph} is as in the input,
#' \code{DFNET_trees} are the generated trees, \code{DFNET_MODULES} are the
#' modules from which the trees were generated and \code{DFNET_MODULES_AUC} is
#' the area under curve of the trees.
#' @examples
#' \dontrun{
#' DFNET_graph <- DFNET_generate_graph_omics(graph, features, target)
#' DFNET_object <- DFNET(DFNET_graph)
#' # postprocess ...
#' }
DFNET <- function(DFNET_graph, ntrees = 100, niter = 200, init.mtry = NaN) {
    state <- DFNET_init(DFNET_graph[[1]], DFNET_graph[[2]], ntrees = ntrees, walk.depth = init.mtry)
    state <- DFNET_iterate(state, niter = niter)
    return(list(
        DFNET_graph = DFNET_graph, DFNET_trees = state$trees,
        DFNET_MODULES = state$modules.nodes, DFNET_MODULES_AUC = state$modules.auc
    ))
}
