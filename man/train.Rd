% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{train}
\alias{train}
\title{Model training}
\usage{
train(
  forest,
  graph,
  features,
  target,
  niter = 200,
  offset = 0,
  min.walk.depth = 2,
  ntrees = 100,
  initial.walk.depth = NaN,
  performance = NULL,
  flatten.sep = "$",
  importance = "impurity",
  splitrule = "gini"
)
}
\arguments{
\item{forest}{a \code{DFNET.forest} or \code{null}.}

\item{graph}{The graph to train the network on.}

\item{features}{numeric matrix or 3D array. The features to train on.}

\item{target}{numeric vector. The target to train towards.}

\item{niter}{integer. The number of iterations to run.}

\item{offset}{integer. An offset added to the iteration count for logging
purposes.}

\item{min.walk.depth}{The integer minimal number of nodes to visit per tree
per iteration.}

\item{ntrees}{integer. The number of trees to generate per iteration.}

\item{initial.walk.depth}{integer. The number of nodes to visit per tree
during initialization.}

\item{performance}{unary function. Called with a decision tree as argument to
estimate that tree's performance.}

\item{flatten.sep}{string. Separator to use when flattening features.}

\item{importance}{variable importance mode.
See \link{ranger:ranger}{ranger::ranger}.}

\item{splitrule}{Splitting rule.
See \link{ranger:ranger}{ranger::ranger}.}
}
\description{
Trains a decision forest on \code{feature} and \code{target}.
}
\details{
This function generates \code{ntrees} modules and decision trees per iteration
and greedily selects those which improve the \code{performance} metric.
The trees are trained on \code{features} and \code{target}.
\code{performance} can use its own validation set, or default to the
\code{features} and \code{target} above (the default), in which case ranger
handles the data split.

In each iteration, this function tries to shrink modules which have
previously been improved.  \code{initial.walk.depth} thus gives the maximal
module size, whereas \code{min.walk.depth} specifies the smallest walk depth.

Model training can be resumed from an already trained forest, in which case
the attributes of that forest are used in lieu of \code{ntrees} and
\code{initial.walk.depth}.  When resuming this training, it might make sense
to also specify the \code{offset} parameter for somewhat improved logging.

The returned \code{DFNET.forest} is a list of shape (\code{trees},
\code{modules}, \code{modules.weights}), where \code{trees} are the decision
trees created for detected \code{modules}, and \code{modules.weights} gives
the weights used for each node.

As "private" attributes used for iteration, \code{generation_size} is set to
\code{ntrees}, \code{walk.depth} captures the walk depth for the next
iteration, and \code{last.performance} to a vector of length \code{ntrees},
containing the result of \code{performance} of each tree w.r.t. \code{target}.
}
\examples{
\dontrun{
forest <- NULL
offset <- 0
while (keep_iterating(forest, target)) { # insert your own iteration criteria
    forest <- train(
        forest,
        graph,
        features,
        niter = 10,
        offset = offset
        # ...
    )
    offset <- offset + 10
}
}

}
